################################################################################
# AGENTICS LLM CONFIGURATION
################################################################################
# This file contains all the environment variables needed to configure LLM
# providers for use with Agentics. All LLM providers are optional - configure
# only the ones you need. The system will auto-discover which LLMs are available
# based on the environment variables present.
################################################################################


################################################################################
# GEMINI (Optional)
################################################################################
GEMINI_API_KEY=
GEMINI_MODEL_ID="gemini/gemini-2.0-flash"


################################################################################
# OPENAI (Optional)
################################################################################
OPENAI_API_KEY=
OPENAI_MODEL_ID="openai/gpt-4"


################################################################################
# OPENAI-COMPATIBLE LLMs (Optional)
#
# Configure OpenAI-compatible API providers with custom prefixes.
#
# Supported patterns:
# - Default variant: OPENAI_COMPATIBLE_*
# - Custom variants: OPENAI_COMPATIBLE_<VARIANT>_*
#
# Each variant requires three variables:
# - <PREFIX>_API_KEY: Your API key
# - <PREFIX>_MODEL_ID: The model identifier
# - <PREFIX>_BASE_URL: The base URL of the API
#
# Examples:
################################################################################

# Default OpenAI-compatible variant
OPENAI_COMPATIBLE_API_KEY=
OPENAI_COMPATIBLE_MODEL_ID=
OPENAI_COMPATIBLE_BASE_URL=


################################################################################
# WATSONX (Optional)
################################################################################
MODEL_ID=watsonx/meta-llama/llama-3-3-70b-instruct
WATSONX_URL=https://us-south.ml.cloud.ibm.com
WATSONX_APIKEY=
WATSONX_PROJECTID=


################################################################################
# VLLM - vLLM Server (Optional)
#
# vLLM is an open-source LLM serving framework.
# Configure if you have a vLLM server running locally or remotely.
################################################################################

# Required: vLLM server URL
VLLM_URL=http://localhost:8000/v1

# Optional: Specific model ID for vLLM
VLLM_MODEL_ID="meta-llama/Llama-3.3-70B-Instruct"


################################################################################
# OLLAMA (Optional)
#
# Ollama is an open-source LLM runner that can run models locally.
# Configure if you have Ollama installed and running.
################################################################################

# The model to use with Ollama
# OLLAMA_MODEL_ID="deepseek-r1:latest"
# OLLAMA_MODEL_ID="llama2:latest"
# OLLAMA_MODEL_ID="mistral:latest"


################################################################################
# LITELLM (Optional)
#
# LiteLLM provides access to 100+ LLM providers through a unified interface.
# Use model format: "litellm/provider/model-name"
# See: https://docs.litellm.ai/docs/providers
################################################################################

# Model in format: litellm/provider/model-name
# LITELLM_MODEL="litellm/openai/gpt-4"
# LITELLM_MODEL="litellm/anthropic/claude-3-sonnet"
# LITELLM_MODEL="litellm/cohere/command-r"

# Optional temperature and top_p settings
# LITELLM_TEMPERATURE=0.8
# LITELLM_TOP_P=0.9


################################################################################
# LITELLM PROXY (Optional)
#
# LiteLLM Proxy allows you to run a local proxy server that manages API keys
# and provides a unified interface for multiple LLM providers. In some
# organizations this also provides a way to centralize frontier LLM
# access to employees.
#
################################################################################

# Required: Proxy server URL (typically http://localhost:4000 if running locally, or
https://litellm.your.organization/, no v1)
# LITELLM_PROXY_URL=http://localhost:4000

# Required: API key for proxy authentication
# LITELLM_PROXY_API_KEY=

################################################################################
# Models
# Once you've defined the above environment variables, you can run this command
# to check the available models.
#   uvx --from litellm[proxy] litellm-proxy models list
#
# This command will display all available models configured on your proxy.
################################################################################

# Required: Model name in format: litellm_proxy/<model_name>
# Must start with "litellm_proxy/"
# LITELLM_PROXY_MODEL="litellm_proxy/gpt-4"
# LITELLM_PROXY_MODEL="litellm_proxy/claude-3-sonnet"

# Optional temperature and top_p settings
# LITELLM_PROXY_TEMPERATURE=0.8
# LITELLM_PROXY_TOP_P=0.9
