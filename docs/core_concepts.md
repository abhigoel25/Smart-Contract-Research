# ğŸ§  Core Concepts

Agentics is built around a small set of concepts that work together:

- **Pydantic types** â€“ how you describe structured data  
- **Transducible functions** â€“ LLM-powered, type-safe transformations  
- **Typed state containers (AGs)** â€“ collections of typed rows/documents  
- **Logical Transduction Algebra (LTA)** â€“ the formal backbone  
- **Mapâ€“Reduce** â€“ the execution pattern for large workloads  

This page gives you the mental model you need before diving into code.

---

## 1. Pydantic Types: Describing Structured Data ğŸ“

At the heart of Agentics is the idea that **everything is a type**.

You describe your data using **Pydantic models**:

```python
from pydantic import BaseModel

class Product(BaseModel):
    id: str | None = None
    title: str | None = None
    description: str | None = None
    price: float | None = None
```

These models serve three roles:

1. **Schema** â€“ they define the fields, types, and optionality  
2. **Validation** â€“ they validate inputs and outputs at runtime  
3. **Contract** â€“ they act as the contract between your code and the LLM  

In Agentics, any LLM-powered transformation is expressed as:

> â€œGiven a `Source` type, produce a `Target` type.â€

Instead of prompt engineering around raw strings, you define **transformations between types**.

---

## 2. Transducible Functions: Typed LLM Transformations âš™ï¸

A **transducible function** is the core abstraction in Agentics.

Informally:

> A transducible function is an LLM-backed function  
> that maps inputs of type `Source` to outputs of type `Target`  
> under a set of instructions and constraints.

Conceptually:

```text
Target << Source
```

Example:

```python
from pydantic import BaseModel

class Review(BaseModel):
    text: str

class ReviewSummary(BaseModel):
    sentiment: str
    summary: str
```

A transducible function might be:

```python
fn: (Review) -> ReviewSummary
```

with instructions like:

> â€œGiven a review, detect its sentiment (positive/negative/neutral) and produce a one-sentence summary.â€

Key properties:

- **Typed I/O** â€“ the function is bound to `Source` and `Target` Pydantic models.  
- **Single Source of Truth for Instructions** â€“ instructions live alongside the function definition.  
- **LLM-Agnostic** â€“ the function describes *what* to transform; the underlying model can change.  
- **Composable** â€“ functions can be chained, branched, or merged into larger workflows.

You donâ€™t call the LLM directly; you **call the transducible function**, which manages LLM calls, validation, retries, and evidence tracking.

---

## 3. Typed State Containers (AGs): Working with Collections ğŸ—‚ï¸

Transformations rarely happen on a single object. You typically work with **collections** of items (rows, documents, events, etc.).

Agentics introduces **typed state containers** (AG) to:

- Hold a collection of instances of a given Pydantic type  
- Preserve that type information across operations  
- Provide a uniform interface for Mapâ€“Reduce, filtering, joining, etc.


Conceptually, you can think of an `AG[Source]` like a type-aware table:
 

```text
AG[Review]
  â”œâ”€ row 0: Review(text="â€¦")
  â”œâ”€ row 1: Review(text="â€¦")
  â””â”€ row n: Review(text="â€¦")
```

Applying a transducible function `(Review) -> ReviewSummary` over an `AG[Review]` conceptually yields an `AG[ReviewSummary]`.

Typed state containers give you:

- **Clarity** â€“ you always know what type youâ€™re holding.  
- **Safety** â€“ operations can check types and schemas instead of guessing.  
- **Composability** â€“ containers can flow between functions and stages.

You can think of state containers as the **data plane** of Agentics.


Note: The name Agentics is derived as a legacy from the first version of Agentics, in which data models and transformations were blended into the same object. By introducing transducible functions as first class citizens, Agentics 2.0 uses AGs primarily as a data structure, although it is still possible to use them directly for transformations. See agentics v1.0 documentation to learn more. 


---

## 4. Logical Transduction Algebra (LTA): The Formal Backbone ğŸ“š

Transducible functions and typed states are not just coding patterns; they are backed by a formal framework called **Logical Transduction Algebra (LTA)**.

You do **not** need to understand the full mathematics to use Agentics, but the intuition is important:

- **Transductions as Morphisms**  
  Each transducible function is treated as a morphism between types:  
  `Source âŸ¶ Target`.

- **Composability**  
  If you have `f: A âŸ¶ B` and `g: B âŸ¶ C`, then you can form a composite transduction `g âˆ˜ f: A âŸ¶ C`. Agentics gives you a practical way to do this over LLM-based functions.


- **Explainability & Evidence**  
  Because transductions are modeled as structured mappings, Agentics can track **which fields** and **which steps** contributed to the final outputs. This underpins **evidence tracking** and **traceability**.

In short:

> LTA provides the theoretical foundation  
> for why your pipelines are composable and explainable,  
> even though they are powered by probabilistic models.

---

## 5. Mapâ€“Reduce: Scaling Transductions ğŸš€

Once you have:

- Typed collections (`AG[Source]`), and  
- Typed transformations (`Source -> Target`),

you need a way to run these at scale. Agentics uses a familiar pattern: **Mapâ€“Reduce**.

### 5.1 Map Phase (`amap`)

The **map** phase applies a transducible function to each element (or batch) of a collection.

Conceptually:

```text
list[Source]  --amap(f)-->  list[Target]
```

Where `f: Source -> Target`.

Properties:

- **Parallelizable** â€“ each element can be processed independently.  
- **Asynchronous** â€“ `amap` is designed for async I/O and concurrent execution.  
- **Typed In/Out** â€“ both input and output containers carry their types.

Typical use cases:

- Extracting structured info from documents  
- Enriching rows with LLM-derived attributes  
- Normalizing or cleaning text fields at scale  

### 5.2 Reduce Phase (`areduce`)

The **reduce** phase aggregates a collection back into a smaller structure (often a single summary or global view).

```text
list[Target]  --areduce(g)-->  GlobalSummary
```

Where `g` is a transducible function or aggregation operation that takes many items and produces fewer (often one).

Examples:

- Summarizing a whole dataset into a report object  
- Producing global statistics or flags  
- Clustering and relation induction 

Mapâ€“Reduce in Agentics is a **logical pattern**, not tied to any specific infrastructure:

- `amap` = â€œapply a typed transformation to many itemsâ€  
- `areduce` = â€œaggregate many results into fewer structured outputsâ€

Together, they define how **large-scale reasoning workflows** are expressed in Agentics.

---

## 6. How the Concepts Fit Together ğŸ”—

A typical workflow looks like this:

1. **Define your types**  
   Use Pydantic to describe your raw data (`Source`) and desired outputs (`Target`, `Report`, etc.).

2. **Define transducible functions**  
   For each logical step, define a transducible function:  
   extraction â†’ normalization â†’ classification â†’ enrichment â†’ summarization.

3. **Load data into typed state containers (Optional)**  
   Wrap your dataset into a container such as `AG[Source]`. 
   You can also use simple python lists of objects of the intended type. 


4. **Apply Mapâ€“Reduce**  
   - Use `amap` to apply transducible functions over the collection. 
   - Use `areduce` to build global summaries or reports.

5. **Rely on LTA properties**  
   Because everything is a typed transduction, you can:  
   - Compose steps cleanly,  
   - Trace outputs back to inputs,  
   - Reason about structure and invariants in your pipeline.

---

## 7. Summary âœ…

- **Pydantic types** give you schemas and validation.  
- **Transducible functions** turn LLM calls into typed, reusable transformations.  
- **Typed state containers** hold collections of those types with clear semantics.  
- **Logical Transduction Algebra (LTA)** explains why these transformations compose and remain interpretable.  
- **Mapâ€“Reduce** provides the pattern for scaling these transductions to large datasets.

From here, you can explore:

- ğŸ‘‰ [Transducible Functions](transducible_functions.md) for concrete examples of defining and using transducible functions
- ğŸ‘‰ `types_and_states.md` for data modeling patterns  
- ğŸ‘‰ `mapreduce.md` to see how large-scale execution works in practice  
